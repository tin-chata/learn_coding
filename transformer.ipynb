{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"tensor_shapes\")\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "# if you want the model to continuously print tensor shapes, set to DEBUG!\n",
    "logger.setLevel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def getclass():\n",
    "    stack = inspect.stack()\n",
    "    return stack[3][0].f_locals[\"self\"].__class__\n",
    "\n",
    "# A helper function to check how tensor sizes change\n",
    "def log_size(tsr: torch.Tensor, name: str):\n",
    "    cls = getclass()\n",
    "    logger.log(level=cls.level, msg=f\"[{cls.__name__}] {name} size={tsr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "# Control how much debugging output we want\n",
    "class TensorLoggingLevels(IntEnum):\n",
    "    attention = 1\n",
    "    attention_head = 2\n",
    "    multihead_attention_block = 3\n",
    "    enc_dec_block = 4\n",
    "    enc_dec = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    level = 1\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.weight = nn.Parameter(pe, requires_grad=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         return self.weight[:, :x.size(1), :] # (1, Seq, Feature)\n",
    "        return self.weight[:, :x.size(1), :].expand(x.size(0), -1, -1) # (Batch, Seq, Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 30, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posemb = PositionalEmbedding(512)\n",
    "x = torch.randint(1000, (5, 30)).to(dtype=torch.long)\n",
    "posemb(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPositionEmbedding(nn.Module):\n",
    "    level = 1\n",
    "    def __init__(self, vocab_size, d_model=512):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.LongTensor, mask=None) -> torch.FloatTensor:\n",
    "        return self.word_embedding(x) + self.position_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 30, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = WordPositionEmbedding(1000)\n",
    "x = torch.randint(1000, (5, 30)).to(dtype=torch.long)\n",
    "emb(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.attention # Logging level: \n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q, k, v: (Batch, Seq, Feature)\n",
    "        d_k = k.size(-1) # get the size of the key\n",
    "        assert q.size(-1) == d_k\n",
    "\n",
    "        # compute the dot product between queries and keys for\n",
    "        # each batch and position in the sequence\n",
    "        # (Batch, Seq, Feature) x (Batch, Feature, Seq) --> (Batch, Seq, Seq)\n",
    "        attn = torch.bmm(q, k.transpose(Dim.seq, Dim.feature)) # (Batch, Seq, Seq)\n",
    "        # we get an attention score between each position in the sequence\n",
    "        # for each batch\n",
    "\n",
    "        # scale the dot products by the dimensionality (see the paper for why we do this!)\n",
    "        attn = attn / math.sqrt(d_k)\n",
    "        # normalize the weights across the sequence dimension\n",
    "        # (Note that since we transposed, the sequence and feature dimensions are switched)\n",
    "        attn = torch.exp(attn)\n",
    "        log_size(attn, \"attention weight\") # (Batch, Seq, Seq)\n",
    "        \n",
    "        # fill attention weights with 0s where padded\n",
    "        if mask is not None: attn = attn.masked_fill(mask, 0)\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "        attn = self.dropout(attn)\n",
    "        # (Batch, Seq, Seq) x (Batch, Seq, Feature) --> (Batch, Seq, Feature)\n",
    "        output = torch.bmm(attn, v) # (Batch, Seq, Feature)\n",
    "        log_size(output, \"attention output size\") # (Batch, Seq, Feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4673, 0.4940, 0.5537, 0.3861, 0.4264, 0.5436, 0.5173, 0.4326,\n",
       "          0.4225, 0.4436, 0.5345, 0.5865, 0.5043, 0.4486, 0.5436, 0.5258,\n",
       "          0.5037, 0.5831, 0.5165, 0.5009],\n",
       "         [0.4826, 0.5054, 0.6466, 0.4544, 0.5262, 0.6494, 0.6370, 0.5545,\n",
       "          0.5133, 0.5524, 0.5364, 0.7067, 0.5404, 0.5767, 0.6499, 0.5754,\n",
       "          0.5823, 0.6228, 0.5902, 0.5283],\n",
       "         [0.4985, 0.5123, 0.6367, 0.4539, 0.5270, 0.6612, 0.6483, 0.5564,\n",
       "          0.5258, 0.5484, 0.5416, 0.7046, 0.5314, 0.5768, 0.6504, 0.5703,\n",
       "          0.5723, 0.6296, 0.5947, 0.5329],\n",
       "         [0.4664, 0.4985, 0.6478, 0.4616, 0.5296, 0.6523, 0.6348, 0.5491,\n",
       "          0.5042, 0.5672, 0.5341, 0.7246, 0.5641, 0.5575, 0.6537, 0.5500,\n",
       "          0.5785, 0.6062, 0.5835, 0.5259],\n",
       "         [0.4653, 0.5070, 0.6525, 0.4521, 0.5288, 0.6490, 0.6315, 0.5436,\n",
       "          0.5084, 0.5625, 0.5314, 0.7196, 0.5618, 0.5590, 0.6638, 0.5535,\n",
       "          0.5744, 0.6045, 0.5793, 0.5326],\n",
       "         [0.4655, 0.5098, 0.5525, 0.4006, 0.5220, 0.5858, 0.5614, 0.5342,\n",
       "          0.4451, 0.5085, 0.4777, 0.6409, 0.5416, 0.4809, 0.6045, 0.4736,\n",
       "          0.5081, 0.5203, 0.5299, 0.4773],\n",
       "         [0.4945, 0.4948, 0.6400, 0.4639, 0.5176, 0.6628, 0.6488, 0.5502,\n",
       "          0.5210, 0.5442, 0.5457, 0.7120, 0.5296, 0.5707, 0.6349, 0.5758,\n",
       "          0.5766, 0.6384, 0.5908, 0.5274],\n",
       "         [0.3700, 0.3383, 0.4617, 0.3930, 0.4219, 0.4972, 0.4831, 0.4533,\n",
       "          0.3724, 0.4784, 0.3989, 0.5370, 0.4256, 0.4719, 0.4316, 0.4823,\n",
       "          0.5334, 0.4992, 0.4960, 0.3498],\n",
       "         [0.4828, 0.5235, 0.6404, 0.4472, 0.5493, 0.6637, 0.6325, 0.5627,\n",
       "          0.5157, 0.5634, 0.5299, 0.6982, 0.5633, 0.5636, 0.6636, 0.5643,\n",
       "          0.5827, 0.6076, 0.5933, 0.5149],\n",
       "         [0.4309, 0.4397, 0.6454, 0.4377, 0.4873, 0.6229, 0.6130, 0.5323,\n",
       "          0.4687, 0.4921, 0.5277, 0.6588, 0.4776, 0.5583, 0.6021, 0.5561,\n",
       "          0.5383, 0.6038, 0.5672, 0.4556]],\n",
       "\n",
       "        [[0.3457, 0.3458, 0.5689, 0.3915, 0.4062, 0.2468, 0.4765, 0.4354,\n",
       "          0.5974, 0.5393, 0.5988, 0.3255, 0.3214, 0.4520, 0.5129, 0.4625,\n",
       "          0.3013, 0.3403, 0.4317, 0.4216],\n",
       "         [0.3503, 0.3269, 0.6978, 0.5190, 0.5199, 0.2243, 0.5604, 0.4697,\n",
       "          0.6170, 0.6158, 0.7015, 0.4353, 0.4361, 0.4755, 0.5053, 0.4831,\n",
       "          0.4727, 0.3774, 0.4781, 0.4354],\n",
       "         [0.3793, 0.4058, 0.7482, 0.5451, 0.5657, 0.3265, 0.5974, 0.5562,\n",
       "          0.7129, 0.6810, 0.7708, 0.4417, 0.4685, 0.5279, 0.5639, 0.5460,\n",
       "          0.4846, 0.4133, 0.5550, 0.5434],\n",
       "         [0.3171, 0.3276, 0.5932, 0.3893, 0.4374, 0.2446, 0.4649, 0.4144,\n",
       "          0.5972, 0.5824, 0.6404, 0.3108, 0.2884, 0.4998, 0.5002, 0.4135,\n",
       "          0.3069, 0.3713, 0.4719, 0.5267],\n",
       "         [0.3902, 0.4253, 0.7254, 0.5312, 0.5616, 0.3312, 0.5828, 0.5464,\n",
       "          0.7276, 0.6862, 0.7619, 0.4362, 0.4715, 0.5334, 0.5721, 0.5438,\n",
       "          0.4711, 0.4088, 0.5744, 0.5610],\n",
       "         [0.3910, 0.4218, 0.7367, 0.5365, 0.5687, 0.3279, 0.5790, 0.5331,\n",
       "          0.7330, 0.6725, 0.7802, 0.4342, 0.4695, 0.5419, 0.5698, 0.5415,\n",
       "          0.4847, 0.4118, 0.5601, 0.5616],\n",
       "         [0.4019, 0.4194, 0.7105, 0.5349, 0.5769, 0.3298, 0.5881, 0.5368,\n",
       "          0.7341, 0.6957, 0.7729, 0.4447, 0.4876, 0.5298, 0.5648, 0.5360,\n",
       "          0.4827, 0.4003, 0.5807, 0.5498],\n",
       "         [0.3162, 0.3443, 0.4693, 0.4758, 0.4048, 0.2888, 0.4130, 0.3708,\n",
       "          0.5347, 0.4343, 0.5213, 0.3032, 0.4115, 0.3528, 0.4191, 0.5018,\n",
       "          0.3624, 0.2150, 0.4066, 0.3501],\n",
       "         [0.3664, 0.2894, 0.5406, 0.3510, 0.5209, 0.3045, 0.4559, 0.4139,\n",
       "          0.5761, 0.5756, 0.7205, 0.3645, 0.3493, 0.4644, 0.4405, 0.3571,\n",
       "          0.3218, 0.3574, 0.4021, 0.4256],\n",
       "         [0.3885, 0.4257, 0.7280, 0.5390, 0.5709, 0.3384, 0.5763, 0.5377,\n",
       "          0.7320, 0.6717, 0.7743, 0.4345, 0.4803, 0.5315, 0.5619, 0.5427,\n",
       "          0.4894, 0.4033, 0.5653, 0.5600]],\n",
       "\n",
       "        [[0.7184, 0.4634, 0.3396, 0.5573, 0.5426, 0.3529, 0.5412, 0.3526,\n",
       "          0.3631, 0.5003, 0.3452, 0.3982, 0.5095, 0.4013, 0.2857, 0.6081,\n",
       "          0.5339, 0.4402, 0.5071, 0.3605],\n",
       "         [0.7866, 0.5480, 0.3754, 0.5905, 0.6974, 0.4963, 0.6673, 0.5023,\n",
       "          0.4414, 0.6725, 0.4283, 0.5569, 0.6750, 0.5271, 0.4212, 0.7778,\n",
       "          0.6700, 0.5700, 0.7044, 0.4537],\n",
       "         [0.4669, 0.3510, 0.2653, 0.3991, 0.4257, 0.3361, 0.3867, 0.4049,\n",
       "          0.2425, 0.5304, 0.2569, 0.3762, 0.4475, 0.3171, 0.2240, 0.5439,\n",
       "          0.3846, 0.2589, 0.4631, 0.3545],\n",
       "         [0.7676, 0.5567, 0.3752, 0.5866, 0.6975, 0.5123, 0.6728, 0.5064,\n",
       "          0.4483, 0.7035, 0.4392, 0.5746, 0.6845, 0.5272, 0.4369, 0.7632,\n",
       "          0.6516, 0.5700, 0.6963, 0.4589],\n",
       "         [0.6038, 0.4838, 0.2994, 0.5327, 0.5545, 0.4306, 0.4923, 0.4454,\n",
       "          0.3405, 0.6512, 0.3906, 0.3962, 0.5208, 0.3513, 0.2713, 0.6570,\n",
       "          0.4714, 0.4231, 0.6162, 0.4099],\n",
       "         [0.7825, 0.5530, 0.3772, 0.5937, 0.6914, 0.4959, 0.6816, 0.5013,\n",
       "          0.4449, 0.6831, 0.4288, 0.5626, 0.6802, 0.5221, 0.4224, 0.7740,\n",
       "          0.6538, 0.5738, 0.7062, 0.4592],\n",
       "         [0.7336, 0.5108, 0.3493, 0.5735, 0.5731, 0.3790, 0.6500, 0.4416,\n",
       "          0.3855, 0.5955, 0.3477, 0.4806, 0.6128, 0.4691, 0.3318, 0.7119,\n",
       "          0.5518, 0.5027, 0.6092, 0.4550],\n",
       "         [0.6831, 0.4801, 0.3702, 0.5139, 0.6039, 0.4215, 0.6321, 0.4720,\n",
       "          0.4160, 0.6321, 0.3240, 0.5566, 0.6448, 0.4876, 0.3799, 0.7150,\n",
       "          0.5951, 0.4679, 0.6169, 0.4371],\n",
       "         [0.6970, 0.5493, 0.3316, 0.5886, 0.6083, 0.4622, 0.6007, 0.4492,\n",
       "          0.3822, 0.6834, 0.4076, 0.5006, 0.6012, 0.4588, 0.3603, 0.6983,\n",
       "          0.5365, 0.4879, 0.6335, 0.4569],\n",
       "         [0.6915, 0.4898, 0.3041, 0.4926, 0.6504, 0.4576, 0.5833, 0.4575,\n",
       "          0.3993, 0.5869, 0.4053, 0.5030, 0.6146, 0.5184, 0.4180, 0.6879,\n",
       "          0.6418, 0.5502, 0.6229, 0.3908]],\n",
       "\n",
       "        [[0.3139, 0.4097, 0.5317, 0.3140, 0.3645, 0.4138, 0.5157, 0.4824,\n",
       "          0.4731, 0.6607, 0.3690, 0.3391, 0.3695, 0.4789, 0.4155, 0.4143,\n",
       "          0.3424, 0.4531, 0.4716, 0.4706],\n",
       "         [0.3370, 0.5234, 0.5562, 0.3671, 0.4319, 0.4642, 0.5119, 0.5692,\n",
       "          0.4948, 0.7087, 0.4329, 0.4116, 0.5480, 0.4468, 0.6320, 0.4460,\n",
       "          0.3848, 0.4543, 0.5925, 0.4704],\n",
       "         [0.3348, 0.5292, 0.5533, 0.3627, 0.4293, 0.4662, 0.5186, 0.5846,\n",
       "          0.5001, 0.7079, 0.4428, 0.4126, 0.5466, 0.4510, 0.6452, 0.4459,\n",
       "          0.3935, 0.4593, 0.5914, 0.4695],\n",
       "         [0.3263, 0.4842, 0.5626, 0.4286, 0.3960, 0.5303, 0.5552, 0.5195,\n",
       "          0.5597, 0.7619, 0.4720, 0.4483, 0.4534, 0.5055, 0.5460, 0.5314,\n",
       "          0.3638, 0.5167, 0.5902, 0.5011],\n",
       "         [0.3066, 0.5079, 0.4850, 0.3547, 0.3263, 0.4866, 0.5528, 0.5878,\n",
       "          0.5912, 0.7078, 0.4918, 0.4091, 0.4156, 0.4849, 0.5992, 0.4818,\n",
       "          0.3903, 0.5023, 0.5173, 0.4254],\n",
       "         [0.3742, 0.5623, 0.5988, 0.4567, 0.4495, 0.5360, 0.6025, 0.6118,\n",
       "          0.6128, 0.8360, 0.5023, 0.4418, 0.5530, 0.5238, 0.6215, 0.5497,\n",
       "          0.4236, 0.5423, 0.6521, 0.5323],\n",
       "         [0.2656, 0.4774, 0.4398, 0.2843, 0.3072, 0.4130, 0.4573, 0.5463,\n",
       "          0.4690, 0.5683, 0.4322, 0.3633, 0.4280, 0.4095, 0.6012, 0.3747,\n",
       "          0.3467, 0.4217, 0.4552, 0.3735],\n",
       "         [0.3659, 0.5853, 0.5962, 0.4718, 0.4362, 0.5375, 0.6097, 0.6401,\n",
       "          0.6186, 0.8123, 0.4951, 0.4377, 0.5626, 0.5315, 0.6547, 0.5736,\n",
       "          0.3965, 0.5432, 0.6245, 0.5181],\n",
       "         [0.3524, 0.5643, 0.5845, 0.4514, 0.4457, 0.5385, 0.6083, 0.6450,\n",
       "          0.6142, 0.8139, 0.5092, 0.4473, 0.5619, 0.5091, 0.6553, 0.5508,\n",
       "          0.4279, 0.5475, 0.6309, 0.5145],\n",
       "         [0.3597, 0.5644, 0.5949, 0.4645, 0.4543, 0.5474, 0.6054, 0.6394,\n",
       "          0.6057, 0.8140, 0.5074, 0.4525, 0.5627, 0.5233, 0.6452, 0.5519,\n",
       "          0.4142, 0.5492, 0.6404, 0.5337]],\n",
       "\n",
       "        [[0.5151, 0.4700, 0.5678, 0.7480, 0.4312, 0.5287, 0.6554, 0.7341,\n",
       "          0.5942, 0.7949, 0.4599, 0.4912, 0.6009, 0.6674, 0.3938, 0.6746,\n",
       "          0.5799, 0.5007, 0.5183, 0.5571],\n",
       "         [0.5099, 0.4059, 0.5357, 0.6531, 0.3607, 0.4764, 0.6497, 0.6736,\n",
       "          0.5826, 0.7185, 0.4394, 0.4133, 0.5366, 0.5843, 0.3779, 0.5964,\n",
       "          0.5175, 0.4106, 0.4305, 0.4806],\n",
       "         [0.4037, 0.4174, 0.4584, 0.6580, 0.3443, 0.4736, 0.5108, 0.6169,\n",
       "          0.4601, 0.6345, 0.3925, 0.4630, 0.4948, 0.5443, 0.3179, 0.5785,\n",
       "          0.4619, 0.4867, 0.4518, 0.4649],\n",
       "         [0.5232, 0.4667, 0.5720, 0.7613, 0.4383, 0.5235, 0.6473, 0.7307,\n",
       "          0.5939, 0.7830, 0.4432, 0.4947, 0.6050, 0.6712, 0.4028, 0.6730,\n",
       "          0.5752, 0.5027, 0.5273, 0.5514],\n",
       "         [0.5263, 0.3945, 0.5396, 0.6705, 0.3701, 0.4716, 0.6409, 0.6663,\n",
       "          0.5770, 0.7187, 0.4422, 0.4108, 0.5299, 0.5990, 0.3820, 0.5949,\n",
       "          0.5195, 0.4011, 0.4353, 0.4945],\n",
       "         [0.4677, 0.3875, 0.5148, 0.6730, 0.4144, 0.4491, 0.5402, 0.6443,\n",
       "          0.4968, 0.7087, 0.4097, 0.3935, 0.5137, 0.5670, 0.3681, 0.6192,\n",
       "          0.5170, 0.4391, 0.5244, 0.5127],\n",
       "         [0.5099, 0.4683, 0.5669, 0.7440, 0.4391, 0.5211, 0.6575, 0.7290,\n",
       "          0.6035, 0.7817, 0.4456, 0.4876, 0.6095, 0.6506, 0.4046, 0.6765,\n",
       "          0.5705, 0.5008, 0.5254, 0.5380],\n",
       "         [0.4146, 0.4176, 0.3780, 0.5901, 0.3694, 0.3958, 0.4605, 0.5437,\n",
       "          0.5208, 0.6097, 0.4387, 0.4296, 0.4708, 0.5469, 0.3487, 0.5172,\n",
       "          0.4629, 0.4048, 0.4082, 0.4685],\n",
       "         [0.3860, 0.4172, 0.4444, 0.5803, 0.3063, 0.4324, 0.4228, 0.5835,\n",
       "          0.5000, 0.5988, 0.3877, 0.4381, 0.4952, 0.5480, 0.3814, 0.4966,\n",
       "          0.4747, 0.4500, 0.4207, 0.4510],\n",
       "         [0.5156, 0.4054, 0.5305, 0.6613, 0.3569, 0.4884, 0.6426, 0.6766,\n",
       "          0.5718, 0.7233, 0.4445, 0.4231, 0.5313, 0.5985, 0.3712, 0.5912,\n",
       "          0.5228, 0.4067, 0.4217, 0.4932]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double Checking\n",
    "attn = ScaledDotProductAttention()\n",
    "q = torch.rand(5, 10, 20)\n",
    "k = torch.rand(5, 10, 20)\n",
    "v = torch.rand(5, 10, 20)\n",
    "attn(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    level = TensorLoggingLevels.attention_head\n",
    "    def __init__(self, d_model, d_feature, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # We will assume the queries, keys, and values all have the same feature size\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.query_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.key_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.value_tfm = nn.Linear(d_model, d_feature)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        Q = self.query_tfm(queries) # (Batch, Seq, Feature)\n",
    "        K = self.key_tfm(keys) # (Batch, Seq, Feature)\n",
    "        V = self.value_tfm(values) # (Batch, Seq, Feature)\n",
    "        log_size(Q, \"queries, keys, vals\")\n",
    "        # compute multiple attention weighted sums\n",
    "        x = self.attn(Q, K, V)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5787, -0.5825,  0.2877,  0.4638,  0.5377, -0.1285, -0.3489,\n",
       "          -0.6006,  0.7705, -0.4411,  0.0052,  0.0720,  0.1320,  0.0365,\n",
       "           0.3778, -0.0909,  0.0165, -0.0921,  0.0472, -0.3238],\n",
       "         [-0.4205, -0.4182,  0.1735,  0.3552,  0.3908, -0.0873, -0.2271,\n",
       "          -0.4572,  0.5092, -0.3006,  0.0291,  0.0855,  0.0959,  0.1057,\n",
       "           0.2389, -0.0284,  0.0358, -0.0488,  0.0508, -0.2354],\n",
       "         [-0.4049, -0.3178,  0.1341,  0.2661,  0.3309, -0.0649, -0.1953,\n",
       "          -0.4050,  0.4349, -0.2690,  0.0220,  0.0628,  0.0675,  0.0876,\n",
       "           0.1906, -0.0455,  0.0312, -0.0278,  0.0283, -0.2013],\n",
       "         [-0.5344, -0.4068,  0.2043,  0.3159,  0.4202, -0.0688, -0.2693,\n",
       "          -0.4689,  0.5893, -0.3714, -0.0221,  0.0391,  0.0928,  0.0064,\n",
       "           0.2935, -0.1110,  0.0135, -0.0469,  0.0099, -0.2521],\n",
       "         [-0.5760, -0.5850,  0.2888,  0.4658,  0.5372, -0.1299, -0.3472,\n",
       "          -0.6040,  0.7707, -0.4398,  0.0082,  0.0771,  0.1337,  0.0419,\n",
       "           0.3773, -0.0877,  0.0189, -0.0908,  0.0482, -0.3247],\n",
       "         [-0.5084, -0.5486,  0.2826,  0.4557,  0.4876, -0.1263, -0.3282,\n",
       "          -0.5612,  0.7113, -0.3877, -0.0066,  0.0309,  0.1040,  0.0145,\n",
       "           0.3479, -0.0884, -0.0072, -0.0792,  0.0890, -0.3126],\n",
       "         [-0.5728, -0.5893,  0.2938,  0.4673,  0.5348, -0.1297, -0.3442,\n",
       "          -0.6034,  0.7721, -0.4382,  0.0083,  0.0810,  0.1363,  0.0440,\n",
       "           0.3765, -0.0869,  0.0198, -0.0895,  0.0485, -0.3239],\n",
       "         [-0.5738, -0.5873,  0.2913,  0.4682,  0.5369, -0.1316, -0.3483,\n",
       "          -0.6064,  0.7738, -0.4384,  0.0092,  0.0758,  0.1325,  0.0428,\n",
       "           0.3761, -0.0874,  0.0182, -0.0912,  0.0516, -0.3257],\n",
       "         [-0.5721, -0.5876,  0.2929,  0.4660,  0.5350, -0.1309, -0.3464,\n",
       "          -0.6031,  0.7739, -0.4384,  0.0091,  0.0789,  0.1348,  0.0425,\n",
       "           0.3765, -0.0876,  0.0195, -0.0915,  0.0485, -0.3235],\n",
       "         [-0.4363, -0.4669,  0.2499,  0.3827,  0.4166, -0.1200, -0.2434,\n",
       "          -0.4916,  0.6124, -0.3368,  0.0241,  0.1184,  0.1269,  0.1113,\n",
       "           0.2746, -0.0562,  0.0502, -0.0910,  0.0391, -0.2435]],\n",
       "\n",
       "        [[-0.5766, -0.6307,  0.2290,  0.2890,  0.5639, -0.1476, -0.3485,\n",
       "          -0.4673,  0.7021, -0.3531,  0.0426,  0.0900,  0.2140, -0.0426,\n",
       "           0.3687, -0.1424,  0.0528, -0.1540,  0.0107, -0.2774],\n",
       "         [-0.5298, -0.6093,  0.2789,  0.3158,  0.5508, -0.1415, -0.3262,\n",
       "          -0.4513,  0.6966, -0.3541,  0.0170,  0.1180,  0.2370, -0.0297,\n",
       "           0.3735, -0.1563,  0.0497, -0.1984, -0.0043, -0.2531],\n",
       "         [-0.4881, -0.4732,  0.1798,  0.2057,  0.4681, -0.1092, -0.2550,\n",
       "          -0.3742,  0.5271, -0.2812, -0.0329,  0.0716,  0.1431, -0.0018,\n",
       "           0.2439, -0.1158,  0.0461, -0.1352, -0.0499, -0.2069],\n",
       "         [-0.5730, -0.6309,  0.2314,  0.2918,  0.5635, -0.1483, -0.3455,\n",
       "          -0.4702,  0.7022, -0.3481,  0.0436,  0.0902,  0.2116, -0.0391,\n",
       "           0.3636, -0.1398,  0.0526, -0.1536,  0.0137, -0.2800],\n",
       "         [-0.5652, -0.6074,  0.2490,  0.3312,  0.5535, -0.1310, -0.3496,\n",
       "          -0.4893,  0.7292, -0.3486,  0.0332,  0.1234,  0.2180,  0.0208,\n",
       "           0.3602, -0.1417,  0.0956, -0.2008,  0.0212, -0.2836],\n",
       "         [-0.5535, -0.5702,  0.2296,  0.2870,  0.5165, -0.1462, -0.3489,\n",
       "          -0.4067,  0.6737, -0.3903,  0.0233,  0.1171,  0.2551, -0.0432,\n",
       "           0.4232, -0.1780,  0.0615, -0.2040, -0.0141, -0.2027],\n",
       "         [-0.5492, -0.5696,  0.2727,  0.3157,  0.5356, -0.1191, -0.3262,\n",
       "          -0.4658,  0.6936, -0.3382,  0.0293,  0.1210,  0.2250, -0.0360,\n",
       "           0.3432, -0.1635,  0.0969, -0.1782,  0.0177, -0.2537],\n",
       "         [-0.6151, -0.6627,  0.2819,  0.3581,  0.6110, -0.1590, -0.3757,\n",
       "          -0.5227,  0.7699, -0.3958,  0.0306,  0.1300,  0.2480, -0.0100,\n",
       "           0.4036, -0.1675,  0.0904, -0.2111,  0.0087, -0.2809],\n",
       "         [-0.5484, -0.5675,  0.2749,  0.3202,  0.5342, -0.1197, -0.3237,\n",
       "          -0.4703,  0.6914, -0.3356,  0.0278,  0.1204,  0.2208, -0.0316,\n",
       "           0.3373, -0.1618,  0.0993, -0.1766,  0.0201, -0.2552],\n",
       "         [-0.5380, -0.5959,  0.2463,  0.3529,  0.5759, -0.1699, -0.3309,\n",
       "          -0.5012,  0.6948, -0.3411,  0.0580,  0.1347,  0.2184,  0.0346,\n",
       "           0.3442, -0.1383,  0.0950, -0.2073,  0.0248, -0.2461]],\n",
       "\n",
       "        [[-0.7853, -0.5474,  0.2045,  0.3940,  0.5463, -0.1005, -0.3011,\n",
       "          -0.6037,  0.6311, -0.6049, -0.0274,  0.1541,  0.1532,  0.1371,\n",
       "           0.4317, -0.1770,  0.1133, -0.0057, -0.0253, -0.2612],\n",
       "         [-0.7090, -0.5326,  0.1841,  0.3375,  0.5167, -0.1033, -0.2412,\n",
       "          -0.5372,  0.5550, -0.5240, -0.0176,  0.1430,  0.1544,  0.1002,\n",
       "           0.3817, -0.1553,  0.0914,  0.0282, -0.0355, -0.2474],\n",
       "         [-0.6559, -0.4062,  0.1713,  0.2800,  0.4065, -0.0532, -0.2468,\n",
       "          -0.4793,  0.5327, -0.4802, -0.0460,  0.1299,  0.1091,  0.1121,\n",
       "           0.3325, -0.1467,  0.0808, -0.0188, -0.0352, -0.2159],\n",
       "         [-0.7060, -0.5293,  0.1846,  0.3354,  0.5127, -0.1012, -0.2402,\n",
       "          -0.5348,  0.5543, -0.5225, -0.0176,  0.1446,  0.1542,  0.1019,\n",
       "           0.3792, -0.1535,  0.0924,  0.0267, -0.0367, -0.2465],\n",
       "         [-0.7317, -0.5028,  0.1785,  0.3578,  0.4929, -0.1129, -0.2703,\n",
       "          -0.5261,  0.5799, -0.5656, -0.0433,  0.1260,  0.1263,  0.1370,\n",
       "           0.4006, -0.1758,  0.0976, -0.0263, -0.0146, -0.2441],\n",
       "         [-0.7855, -0.5479,  0.2047,  0.3943,  0.5469, -0.1004, -0.3022,\n",
       "          -0.6054,  0.6320, -0.6041, -0.0265,  0.1521,  0.1522,  0.1340,\n",
       "           0.4308, -0.1767,  0.1123, -0.0034, -0.0252, -0.2623],\n",
       "         [-0.7867, -0.5496,  0.2046,  0.3946,  0.5481, -0.1035, -0.3009,\n",
       "          -0.6041,  0.6307, -0.6066, -0.0275,  0.1522,  0.1525,  0.1350,\n",
       "           0.4335, -0.1796,  0.1123, -0.0041, -0.0239, -0.2613],\n",
       "         [-0.7135, -0.4851,  0.1853,  0.3558,  0.5234, -0.0968, -0.2813,\n",
       "          -0.5604,  0.5625, -0.5531, -0.0208,  0.1304,  0.1358,  0.1010,\n",
       "           0.3851, -0.1885,  0.0850,  0.0002, -0.0271, -0.2058],\n",
       "         [-0.6914, -0.4841,  0.1908,  0.3565,  0.4641, -0.0517, -0.2801,\n",
       "          -0.5348,  0.5816, -0.5156, -0.0134,  0.1466,  0.1480,  0.1284,\n",
       "           0.3673, -0.1322,  0.1178, -0.0205, -0.0268, -0.2498],\n",
       "         [-0.7157, -0.4636,  0.1900,  0.3457,  0.5060, -0.0780, -0.2700,\n",
       "          -0.5415,  0.5683, -0.5403, -0.0436,  0.1558,  0.1636,  0.1209,\n",
       "           0.3972, -0.1764,  0.0979, -0.0326, -0.0365, -0.1987]],\n",
       "\n",
       "        [[-0.6158, -0.6429,  0.2668,  0.4108,  0.5937, -0.1512, -0.3410,\n",
       "          -0.6074,  0.7191, -0.3872,  0.0741,  0.0566,  0.1159, -0.0007,\n",
       "           0.3077, -0.1024,  0.0274, -0.0728,  0.0249, -0.3090],\n",
       "         [-0.6155, -0.6433,  0.2670,  0.4132,  0.5955, -0.1533, -0.3416,\n",
       "          -0.6106,  0.7207, -0.3883,  0.0753,  0.0558,  0.1152, -0.0004,\n",
       "           0.3083, -0.1020,  0.0264, -0.0725,  0.0262, -0.3102],\n",
       "         [-0.5661, -0.5417,  0.2282,  0.3401,  0.5289, -0.1383, -0.3073,\n",
       "          -0.5502,  0.6612, -0.3509,  0.0667,  0.0482,  0.0966, -0.0033,\n",
       "           0.2765, -0.0900,  0.0287, -0.0737,  0.0039, -0.2830],\n",
       "         [-0.6159, -0.6426,  0.2680,  0.4151,  0.5960, -0.1550, -0.3418,\n",
       "          -0.6133,  0.7199, -0.3893,  0.0744,  0.0550,  0.1139,  0.0007,\n",
       "           0.3071, -0.1019,  0.0258, -0.0728,  0.0256, -0.3104],\n",
       "         [-0.5216, -0.6156,  0.2483,  0.4122,  0.5628, -0.1584, -0.3046,\n",
       "          -0.5549,  0.6507, -0.3633,  0.0792,  0.0666,  0.1163,  0.0047,\n",
       "           0.2964, -0.0813,  0.0194, -0.0625,  0.0358, -0.2738],\n",
       "         [-0.5777, -0.5922,  0.2511,  0.3748,  0.5250, -0.1026, -0.3237,\n",
       "          -0.5573,  0.6755, -0.3333,  0.0606,  0.0237,  0.1056, -0.0333,\n",
       "           0.2861, -0.1151,  0.0184, -0.0500,  0.0452, -0.3127],\n",
       "         [-0.4264, -0.5501,  0.2133,  0.3385,  0.4954, -0.1365, -0.2608,\n",
       "          -0.4669,  0.5770, -0.3272,  0.1242,  0.0801,  0.1203,  0.0046,\n",
       "           0.2791, -0.0656,  0.0142, -0.0599,  0.0612, -0.2484],\n",
       "         [-0.4978, -0.4918,  0.1836,  0.2701,  0.4588, -0.0974, -0.2829,\n",
       "          -0.4580,  0.5925, -0.3048,  0.0605,  0.0451,  0.1029, -0.0259,\n",
       "           0.2591, -0.0609,  0.0303, -0.0525, -0.0165, -0.2616],\n",
       "         [-0.6142, -0.6450,  0.2644,  0.4136,  0.5965, -0.1558, -0.3421,\n",
       "          -0.6101,  0.7221, -0.3880,  0.0786,  0.0564,  0.1161,  0.0005,\n",
       "           0.3094, -0.0991,  0.0258, -0.0718,  0.0281, -0.3100],\n",
       "         [-0.4880, -0.5256,  0.2202,  0.3325,  0.4885, -0.1159, -0.2776,\n",
       "          -0.4952,  0.5509, -0.3362,  0.0493,  0.0387,  0.0939, -0.0143,\n",
       "           0.2551, -0.0812,  0.0147, -0.0483, -0.0012, -0.2626]],\n",
       "\n",
       "        [[-0.5085, -0.5384,  0.3017,  0.4120,  0.5028, -0.1755, -0.2813,\n",
       "          -0.4975,  0.6234, -0.4206,  0.0076,  0.0629,  0.1243,  0.0380,\n",
       "           0.3086, -0.1628,  0.0169, -0.1459,  0.0223, -0.1503],\n",
       "         [-0.5898, -0.5935,  0.3256,  0.4711,  0.5928, -0.1666, -0.3136,\n",
       "          -0.5790,  0.7014, -0.4814, -0.0005,  0.0687,  0.1257,  0.0483,\n",
       "           0.3319, -0.1901,  0.0004, -0.1498,  0.0039, -0.1916],\n",
       "         [-0.6783, -0.6769,  0.3410,  0.4908,  0.6513, -0.1702, -0.3539,\n",
       "          -0.6188,  0.7394, -0.5484, -0.0127,  0.0723,  0.1519,  0.0287,\n",
       "           0.3879, -0.2103,  0.0137, -0.1383, -0.0061, -0.2327],\n",
       "         [-0.6063, -0.6065,  0.2867,  0.4255,  0.6050, -0.1634, -0.3053,\n",
       "          -0.5468,  0.6477, -0.4925,  0.0011,  0.0809,  0.1576,  0.0311,\n",
       "           0.3615, -0.1814,  0.0275, -0.1316, -0.0176, -0.1971],\n",
       "         [-0.5301, -0.5404,  0.2714,  0.4049,  0.5066, -0.1624, -0.3113,\n",
       "          -0.4986,  0.6045, -0.4568,  0.0131,  0.0807,  0.1420,  0.0553,\n",
       "           0.3337, -0.1724,  0.0479, -0.1415,  0.0218, -0.1530],\n",
       "         [-0.5918, -0.5930,  0.3241,  0.4694,  0.5941, -0.1658, -0.3120,\n",
       "          -0.5783,  0.6969, -0.4834, -0.0028,  0.0684,  0.1254,  0.0472,\n",
       "           0.3334, -0.1914, -0.0005, -0.1481,  0.0024, -0.1925],\n",
       "         [-0.6216, -0.6056,  0.3080,  0.4292,  0.5921, -0.1438, -0.3155,\n",
       "          -0.5367,  0.6472, -0.4979, -0.0369,  0.0582,  0.1336,  0.0155,\n",
       "           0.3591, -0.1987,  0.0205, -0.1518, -0.0141, -0.2163],\n",
       "         [-0.4759, -0.4551,  0.2059,  0.3338,  0.4922, -0.1176, -0.2550,\n",
       "          -0.4186,  0.4902, -0.4141, -0.0050,  0.0823,  0.1335,  0.0496,\n",
       "           0.3055, -0.1588,  0.0520, -0.1512, -0.0196, -0.1415],\n",
       "         [-0.5897, -0.4794,  0.2459,  0.3766,  0.4917, -0.1053, -0.2844,\n",
       "          -0.5130,  0.5923, -0.3797, -0.0517,  0.0104,  0.0797,  0.0276,\n",
       "           0.2489, -0.1549,  0.0124, -0.0986, -0.0083, -0.2180],\n",
       "         [-0.6797, -0.6760,  0.3402,  0.4902,  0.6505, -0.1705, -0.3543,\n",
       "          -0.6190,  0.7391, -0.5477, -0.0132,  0.0720,  0.1520,  0.0295,\n",
       "           0.3877, -0.2102,  0.0150, -0.1387, -0.0057, -0.2327]]],\n",
       "       grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double Checking\n",
    "attn_head = AttentionHead(20, 20)\n",
    "attn_head(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.multihead_attention_block\n",
    "    def __init__(self, d_model, d_feature, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_feature = d_feature\n",
    "        self.n_heads = n_heads\n",
    "        # in practice, d_model == d_feature * n_heads\n",
    "        assert d_model == d_feature * n_heads\n",
    "\n",
    "        # Note that this is very inefficient:\n",
    "        # I am merely implementing the heads separately because it is \n",
    "        # easier to understand this way\n",
    "        self.attn_heads = nn.ModuleList([AttentionHead(d_model, d_feature, dropout) for _ in range(n_heads)])\n",
    "        self.projection = nn.Linear(d_feature * n_heads, d_model) \n",
    "    \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        log_size(queries, \"Input queries\")\n",
    "        x = [attn(queries, keys, values, mask=mask) # (Batch, Seq, Feature)\n",
    "             for i, attn in enumerate(self.attn_heads)]\n",
    "        log_size(x[0], \"output of single head\")\n",
    "        \n",
    "        # reconcatenate\n",
    "        x = torch.cat(x, dim=Dim.feature) # (Batch, Seq, D_Feature * n_heads)\n",
    "        log_size(x, \"concatenated output\")\n",
    "        x = self.projection(x) # (Batch, Seq, D_Model)\n",
    "        log_size(x, \"projected output\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 160])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 20])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 160])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 160])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1159,  0.0529, -0.0127,  ..., -0.1988, -0.1469,  0.1303],\n",
       "         [ 0.1197,  0.0322, -0.0224,  ..., -0.1162, -0.1722,  0.1110],\n",
       "         [ 0.0994,  0.0027, -0.0074,  ..., -0.2015, -0.1298,  0.1169],\n",
       "         ...,\n",
       "         [ 0.1295, -0.0002, -0.0089,  ..., -0.2222, -0.1359,  0.0992],\n",
       "         [ 0.1382,  0.0174, -0.0020,  ..., -0.2107, -0.1329,  0.1106],\n",
       "         [ 0.1094,  0.0341, -0.0119,  ..., -0.1909, -0.1696,  0.1347]],\n",
       "\n",
       "        [[ 0.1471, -0.0042,  0.0427,  ..., -0.2365, -0.1100,  0.0143],\n",
       "         [ 0.1612,  0.0047,  0.0437,  ..., -0.2896, -0.1670,  0.0122],\n",
       "         [ 0.1377, -0.0082,  0.0380,  ..., -0.2332, -0.0991, -0.0022],\n",
       "         ...,\n",
       "         [ 0.1294, -0.0110,  0.0371,  ..., -0.2590, -0.1424,  0.0102],\n",
       "         [ 0.1590,  0.0161,  0.0354,  ..., -0.2371, -0.1789,  0.0016],\n",
       "         [ 0.1560, -0.0066,  0.0620,  ..., -0.2245, -0.1363,  0.0147]],\n",
       "\n",
       "        [[ 0.0418,  0.0270, -0.0096,  ..., -0.1850, -0.1378,  0.0812],\n",
       "         [ 0.0811,  0.0068,  0.0021,  ..., -0.1832, -0.2022,  0.0664],\n",
       "         [ 0.0358,  0.0021,  0.0222,  ..., -0.1565, -0.1613,  0.0350],\n",
       "         ...,\n",
       "         [ 0.0437,  0.0045,  0.0137,  ..., -0.1510, -0.0996,  0.0728],\n",
       "         [ 0.0459, -0.0065,  0.0348,  ..., -0.1821, -0.1403,  0.0675],\n",
       "         [ 0.0543,  0.0131,  0.0372,  ..., -0.1673, -0.1593,  0.0703]],\n",
       "\n",
       "        [[ 0.1288,  0.0220,  0.0666,  ..., -0.2616, -0.1284,  0.0917],\n",
       "         [ 0.1159,  0.0050,  0.0626,  ..., -0.2315, -0.1185,  0.0769],\n",
       "         [ 0.1135,  0.0180,  0.0692,  ..., -0.2054, -0.1580,  0.0698],\n",
       "         ...,\n",
       "         [ 0.1055,  0.0284,  0.0688,  ..., -0.1798, -0.1285,  0.0598],\n",
       "         [ 0.1082,  0.0115,  0.0576,  ..., -0.2344, -0.1270,  0.1048],\n",
       "         [ 0.1172,  0.0098,  0.0559,  ..., -0.2469, -0.1107,  0.0930]],\n",
       "\n",
       "        [[ 0.0968,  0.0182,  0.0267,  ..., -0.2333, -0.1992,  0.0544],\n",
       "         [ 0.1178,  0.0338,  0.0256,  ..., -0.2487, -0.2145,  0.0532],\n",
       "         [ 0.0958,  0.0397,  0.0223,  ..., -0.2378, -0.1575,  0.0720],\n",
       "         ...,\n",
       "         [ 0.0937,  0.0231,  0.0244,  ..., -0.2518, -0.2027,  0.0708],\n",
       "         [ 0.1150,  0.0501,  0.0227,  ..., -0.2211, -0.1974,  0.0734],\n",
       "         [ 0.0991,  0.0337,  0.0142,  ..., -0.1924, -0.2310,  0.0627]]],\n",
       "       grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll supress logging from the scaled dot product attention now\n",
    "logger.setLevel(TensorLoggingLevels.attention_head)\n",
    "heads = MultiHeadAttention(20 * 8, 20, 8)\n",
    "heads(q.repeat(1, 1, 8), \n",
    "      k.repeat(1, 1, 8), \n",
    "      v.repeat(1, 1, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-8):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec_block\n",
    "    def __init__(self, d_model=512, d_feature=64, d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.position_wise_feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        log_size(x, \"Encoder block input\")\n",
    "        att = self.attn_head(x, x, x, mask=mask)\n",
    "        log_size(x, \"Attention output\")\n",
    "        # Apply normalization and residual connection\n",
    "        x = x + self.dropout(self.layer_norm1(att))\n",
    "        # Apply position-wise feedforward network\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        log_size(x, \"Feedforward output\")\n",
    "        # Apply normalization and residual connection\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        log_size(x, \"Encoder size output\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EncoderBlock] Encoder block input size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 10, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.7680,  2.0711,  0.3541,  ...,  1.2161,  0.2477,  0.3743],\n",
       "         [ 3.3455,  1.9848,  0.0146,  ...,  1.7872, -0.3102,  0.3357],\n",
       "         [ 3.7358,  2.3758, -0.3364,  ...,  0.3960,  0.3831,  0.2983],\n",
       "         ...,\n",
       "         [ 2.6606,  3.3947,  0.5691,  ...,  0.8397, -0.5506, -1.5812],\n",
       "         [ 3.0163,  2.0350, -0.0419,  ...,  0.5514, -0.1235,  0.9761],\n",
       "         [ 2.4870,  0.6717,  0.2109,  ...,  1.0857, -0.7127, -0.1037]],\n",
       "\n",
       "        [[ 1.9009,  2.2927,  1.8900,  ...,  1.2419, -1.2112,  0.0765],\n",
       "         [ 2.8415,  2.6401,  1.2274,  ...,  1.9445,  0.2086, -0.1449],\n",
       "         [ 1.9334,  2.8163,  1.2456,  ...,  0.8339, -1.2177, -0.1095],\n",
       "         ...,\n",
       "         [ 0.5987,  2.3868,  0.6901,  ...,  0.7395, -0.3681,  0.3130],\n",
       "         [ 1.5982,  2.0932,  1.5629,  ...,  0.8693,  1.5622,  0.0692],\n",
       "         [ 1.9024,  2.3831,  0.3466,  ...,  0.1804,  1.1749,  0.3410]],\n",
       "\n",
       "        [[ 3.5502,  2.3804,  1.3971,  ...,  1.5073, -0.1297,  1.7719],\n",
       "         [ 2.7341,  0.7462,  1.7061,  ...,  0.6196, -1.9778,  0.4809],\n",
       "         [ 2.2565,  0.4204,  1.3057,  ...,  0.8772, -0.2637,  0.8978],\n",
       "         ...,\n",
       "         [ 2.6885,  0.9476,  1.1800,  ...,  1.4780, -1.6790,  0.4465],\n",
       "         [ 2.9379,  3.3200,  1.0812,  ...,  1.0872, -1.9281,  0.0087],\n",
       "         [ 3.5969,  2.1708,  0.1749,  ...,  1.6551, -0.2927,  0.1004]],\n",
       "\n",
       "        [[ 3.2460,  2.5739,  1.0184,  ...,  1.8104,  0.0891, -0.2672],\n",
       "         [ 3.0988,  1.3872,  0.9901,  ...,  0.6075,  0.8423,  0.2961],\n",
       "         [ 4.0765,  2.5644,  0.0762,  ...,  1.3034, -0.4577, -1.2847],\n",
       "         ...,\n",
       "         [ 2.3954,  2.3940,  0.1759,  ...,  0.4584, -0.3964,  0.3060],\n",
       "         [ 2.3381,  0.6676,  2.1724,  ...,  0.4350, -0.5601,  2.5824],\n",
       "         [ 1.9711,  2.1923,  0.8615,  ...,  1.9731, -0.4833,  0.3340]],\n",
       "\n",
       "        [[ 2.1483,  1.4293,  1.7389,  ...,  2.5843,  0.6293,  0.4306],\n",
       "         [ 1.0299,  2.0279,  2.0269,  ...,  1.2104,  0.8865,  2.4933],\n",
       "         [ 2.7723,  1.8487,  1.5904,  ...,  0.2900,  0.2044,  1.1009],\n",
       "         ...,\n",
       "         [ 2.9559,  2.3540, -0.4678,  ...,  1.0861, -0.2207,  0.8737],\n",
       "         [ 2.6363,  1.9063,  2.0650,  ...,  0.9850,  0.4270,  0.5333],\n",
       "         [ 2.2803,  2.4466,  0.2614,  ...,  0.9474,  0.4156,  0.4284]]],\n",
       "       grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll supress logging from the individual attention heads\n",
    "logger.setLevel(TensorLoggingLevels.multihead_attention_block)\n",
    "enc = EncoderBlock()\n",
    "enc(torch.rand(5, 10, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec\n",
    "    def __init__(self, n_blocks=6, d_model=512,\n",
    "                 n_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderBlock(d_model=d_model, d_feature=d_model // n_heads,\n",
    "                         d_ff=d_ff, dropout=dropout)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.FloatTensor, mask=None):\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n"
     ]
    }
   ],
   "source": [
    "emb = WordPositionEmbedding(1000)\n",
    "encoder = TransformerEncoder()\n",
    "enc_out = encoder(emb(torch.randint(1000, (5, 30)).to(dtype=torch.long)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 30, 512])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec_block\n",
    "    def __init__(self, d_model=512, d_feature=64,\n",
    "                 d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.position_wise_feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        self.layer_norm3 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_out, \n",
    "                src_mask=None, tgt_mask=None):\n",
    "        # Apply attention to inputs\n",
    "        att = self.masked_attn_head(x, x, x, mask=src_mask)\n",
    "        x = x + self.dropout(self.layer_norm1(att))\n",
    "        # Apply attention to the encoder outputs and outputs of the previous layer\n",
    "        att = self.attn_head(queries=x, keys=enc_out, values=enc_out, mask=tgt_mask)\n",
    "        x = x + self.dropout(self.layer_norm2(att))\n",
    "        # Apply position-wise feedforward network\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EncoderBlock] Encoder block input size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3143, -3.6150,  2.0685,  ...,  1.0186,  2.1423, -5.2383],\n",
       "         [ 0.4081, -3.9372,  0.3825,  ...,  2.2026,  1.9094, -0.9124],\n",
       "         [ 0.9806, -1.7727,  2.2844,  ...,  0.9288,  1.3480, -4.8447],\n",
       "         ...,\n",
       "         [ 2.1945, -4.2671,  2.0488,  ...,  1.3101,  1.7486, -6.4701],\n",
       "         [ 1.9170, -3.3968, -0.4371,  ...,  2.0384,  1.2802, -4.6823],\n",
       "         [ 0.1921, -3.3898,  1.6817,  ...,  0.6606,  1.4826, -4.2986]],\n",
       "\n",
       "        [[ 1.6475, -3.0126,  0.0400,  ...,  1.4631,  2.0488, -5.1784],\n",
       "         [-0.0093, -3.1619, -0.9516,  ...,  0.8779,  2.0665, -3.4306],\n",
       "         [ 1.6324, -4.3141,  0.1368,  ...,  2.8415,  1.8893, -4.9675],\n",
       "         ...,\n",
       "         [ 1.8956, -4.1262, -0.2359,  ...,  2.1064,  1.9551, -4.3774],\n",
       "         [ 0.6772, -4.0377,  0.6609,  ...,  0.3275,  2.4405, -3.2075],\n",
       "         [ 0.9534, -4.3457,  0.0026,  ...,  0.9398,  2.5656, -5.0483]],\n",
       "\n",
       "        [[ 1.2853, -3.8986,  1.1103,  ...,  2.4212,  1.2662, -4.8599],\n",
       "         [ 1.0998, -3.7637,  0.6068,  ...,  2.5279,  1.7101, -5.6137],\n",
       "         [ 1.1072, -3.7412,  1.8999,  ...,  2.2724,  1.5692, -4.9916],\n",
       "         ...,\n",
       "         [ 1.9967, -3.2998, -0.0669,  ...,  2.4107,  1.3409, -6.1072],\n",
       "         [ 0.5179, -1.9414, -0.3207,  ...,  0.2219,  1.3839, -5.2842],\n",
       "         [ 0.0960, -4.1595,  0.5149,  ...,  1.2930,  2.6214, -4.5914]],\n",
       "\n",
       "        [[ 1.5966, -3.6599, -0.0051,  ...,  0.8293,  1.9682, -4.1291],\n",
       "         [ 0.4909, -3.6543, -0.0728,  ...,  1.5341,  0.4102, -2.9193],\n",
       "         [ 1.7253, -4.0715,  0.4538,  ...,  1.9306, -0.2398, -5.2210],\n",
       "         ...,\n",
       "         [ 0.2253, -4.3425,  0.0103,  ...,  2.1217,  1.7701, -5.6546],\n",
       "         [-0.5613, -3.6278,  0.2658,  ...,  2.7426,  1.5070, -5.9284],\n",
       "         [-0.7179, -3.8055, -1.3603,  ...,  1.8870,  2.0418, -1.7293]],\n",
       "\n",
       "        [[ 0.8828, -3.7632, -0.0157,  ...,  0.1798,  0.6578, -5.1962],\n",
       "         [ 0.5848, -5.4095,  0.6242,  ...,  0.7731,  0.7873, -5.5611],\n",
       "         [ 1.1580, -1.4473,  0.4169,  ...,  2.2132,  0.5379, -5.1945],\n",
       "         ...,\n",
       "         [ 1.8517, -0.7951,  1.7662,  ...,  1.5476,  1.7830, -5.4655],\n",
       "         [ 0.6370, -4.2221,  0.3557,  ...,  0.5818,  2.1799, -2.0692],\n",
       "         [ 2.6055, -3.6484,  2.3591,  ...,  0.7055,  1.1845, -6.0041]]],\n",
       "       grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = DecoderBlock()\n",
    "dec(torch.rand(5, 10, 512), enc(torch.rand(5, 10, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec\n",
    "    def __init__(self, n_blocks=6, d_model=512, d_feature=64,\n",
    "                 d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        self.decoders = nn.ModuleList([\n",
    "            DecoderBlock(d_model=d_model, d_feature=d_model // n_heads,\n",
    "                         d_ff=d_ff, dropout=dropout)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x: torch.FloatTensor, \n",
    "                enc_out: torch.FloatTensor, \n",
    "                src_mask=None, tgt_mask=None):\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x, enc_out, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TransformerDecoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll supress logging from the scaled dot product attention now\n",
    "logger.setLevel(TensorLoggingLevels.enc_dec_block)\n",
    "emb = WordPositionEmbedding(1000)\n",
    "encoder = TransformerEncoder()\n",
    "decoder = TransformerDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_ids = torch.randint(1000, (5, 30)).to(dtype=torch.long)\n",
    "tgt_ids = torch.randint(1000, (5, 30)).to(dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5636e+00, -3.8588e+00,  3.7249e+00,  ..., -4.1889e+00,\n",
       "          -5.6192e+00, -3.0030e+00],\n",
       "         [ 2.8379e+00, -2.3141e+00, -1.5286e-01,  ..., -7.5009e+00,\n",
       "          -2.8948e+00, -1.8489e+00],\n",
       "         [ 3.1479e+00, -2.6509e+00,  2.6539e+00,  ..., -7.9812e+00,\n",
       "          -2.8788e+00, -9.3533e-01],\n",
       "         ...,\n",
       "         [-3.8777e+00,  2.8749e-01,  2.3224e+00,  ..., -8.7214e+00,\n",
       "          -4.9369e+00, -7.5240e-01],\n",
       "         [-1.5537e+00, -9.9530e-01,  3.7730e+00,  ..., -8.0768e+00,\n",
       "          -2.6874e-01, -4.8747e+00],\n",
       "         [ 3.6634e+00, -2.4629e+00,  3.8783e-01,  ..., -9.1722e+00,\n",
       "           1.3492e+00, -7.3836e-02]],\n",
       "\n",
       "        [[-2.3774e-01, -2.4522e+00,  8.6978e+00,  ..., -8.3269e+00,\n",
       "          -3.4548e+00, -1.6867e+00],\n",
       "         [-2.6132e+00,  9.3545e-02,  9.6737e-01,  ..., -3.3069e+00,\n",
       "          -6.2419e+00, -2.3105e+00],\n",
       "         [ 2.3841e-02, -2.9472e+00,  1.4953e-01,  ..., -2.7899e+00,\n",
       "          -4.2259e+00,  6.4982e-01],\n",
       "         ...,\n",
       "         [ 6.3979e-01, -4.9068e+00,  8.3914e+00,  ..., -6.3551e+00,\n",
       "          -4.8744e+00, -1.2972e+00],\n",
       "         [-2.3164e+00, -5.7584e+00,  2.7104e+00,  ..., -5.3464e+00,\n",
       "          -2.5674e+00, -1.5729e+00],\n",
       "         [-3.6167e+00,  5.5202e-01,  9.0787e-01,  ..., -3.8370e+00,\n",
       "          -2.1436e+00, -2.6908e+00]],\n",
       "\n",
       "        [[-1.4442e+00,  1.5436e+00, -1.9970e+00,  ..., -1.9635e+00,\n",
       "          -2.0251e+00, -6.0042e+00],\n",
       "         [ 6.5963e-01, -1.8354e+00,  8.7437e-02,  ..., -5.4362e+00,\n",
       "          -3.6453e+00, -2.7096e+00],\n",
       "         [ 3.1361e+00, -2.8443e+00, -6.3448e-01,  ..., -4.4383e-01,\n",
       "          -4.0091e+00,  4.8687e-01],\n",
       "         ...,\n",
       "         [-2.6443e+00, -3.8765e+00, -1.5480e+00,  ..., -3.3557e+00,\n",
       "          -1.0722e+00,  1.5889e+00],\n",
       "         [-1.9411e+00, -5.7520e+00, -5.6113e+00,  ..., -3.6094e+00,\n",
       "           5.1730e-01, -2.8807e-01],\n",
       "         [ 6.5355e-02, -5.0165e+00, -1.1324e+00,  ...,  5.5442e-01,\n",
       "          -1.9142e+00, -1.5788e-01]],\n",
       "\n",
       "        [[ 1.8293e+00, -2.3147e+00, -3.0190e+00,  ..., -5.1794e+00,\n",
       "           2.5819e+00, -2.1665e+00],\n",
       "         [-2.7302e-01, -7.9884e-01,  5.3314e+00,  ..., -4.7584e+00,\n",
       "          -4.8606e+00, -3.5409e+00],\n",
       "         [-6.7748e-01, -6.6976e+00, -1.4877e+00,  ..., -1.0493e+01,\n",
       "          -7.4576e-01, -2.6651e+00],\n",
       "         ...,\n",
       "         [ 2.7124e+00, -3.5100e+00,  1.3462e+00,  ..., -4.6874e+00,\n",
       "          -1.8348e+00, -3.8085e+00],\n",
       "         [-9.9951e-02, -7.4396e+00, -1.0250e-01,  ..., -1.4641e+00,\n",
       "          -1.1795e+00, -3.0956e+00],\n",
       "         [ 7.4982e-01, -8.1217e+00, -4.4655e+00,  ..., -6.7319e+00,\n",
       "           9.1883e-01, -1.2668e+00]],\n",
       "\n",
       "        [[-3.3782e+00,  1.8793e+00,  4.6794e-01,  ..., -1.9745e+00,\n",
       "          -1.3425e+00,  1.1513e+00],\n",
       "         [-3.3707e+00, -1.2790e+00,  9.3022e-01,  ..., -2.0944e+00,\n",
       "          -3.3770e+00, -6.3283e-01],\n",
       "         [-6.9004e+00, -3.3885e+00, -1.2803e-01,  ..., -6.1730e+00,\n",
       "          -7.5887e-01,  6.5945e-01],\n",
       "         ...,\n",
       "         [-7.1213e+00, -3.4576e-01,  3.7686e+00,  ..., -7.5276e+00,\n",
       "          -4.7762e-01,  1.6181e+00],\n",
       "         [-5.3410e+00, -2.5046e+00,  9.9732e-01,  ..., -6.0822e+00,\n",
       "          -3.9988e+00, -1.6737e+00],\n",
       "         [-7.3602e+00, -1.2683e+00, -1.4196e-01,  ..., -6.0125e+00,\n",
       "          -4.1157e+00,  1.0319e+00]]], grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = encoder(emb(src_ids))\n",
    "decoder(emb(tgt_ids), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 20])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.repeat(1, 1, 8).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
